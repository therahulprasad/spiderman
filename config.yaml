## Debug
debug: false
## Prints URL in terminal
display_matched_url: false

## Name of the project
project: "Crawler Project"
# Type of project (crawl / batch)
project_type: "crawl"
# Directory to store downloaded content
directory: "Project"
# Number of workers
web_count: 100

### Crawler settings
# First URL to crawl
root_url: "http://example.com/"
# Regex to validate if link has to be crawled
link_validator: "^http://example.com/.*"
# Regex to preprocess the link before queuing for example (#.*) will select all characters after # in the url
link_sanitizer: "(#.*)"
# Replacement for selected string from sanitizer
link_sanitizer_replacement: ""

### Batch processing settings
batch_url: ""

### Parser settings
# Check for the content in the page only if this css selector is present, keep it blank to ignore this
page_validator: ""
# CSS Selector of the content to be parsed
content_selector: ".storyBody"
